{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9994dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4098 images belonging to 4 classes.\n",
      "Found 1023 images belonging to 4 classes.\n",
      "Found 1279 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create a new generator\n",
    "imagegen=ImageDataGenerator(rescale=1.0/255,validation_split=0.2)\n",
    "\n",
    "#load train data\n",
    "train=imagegen.flow_from_directory(\"AlzheimersDataset/train\",class_mode='categorical',shuffle=False,batch_size=16,target_size=(176,208),subset=\"training\")\n",
    "\n",
    "#load val data\n",
    "\n",
    "val=imagegen.flow_from_directory(\"AlzheimersDataset/train\",class_mode='categorical',shuffle=False,batch_size=16,target_size=(176,208),subset=\"validation\")\n",
    "\n",
    "#test\n",
    "\n",
    "test=imagegen.flow_from_directory(\"AlzheimersDataset/test\",class_mode='categorical',shuffle=False,batch_size=16,target_size=(176,208))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cdfd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Input,BatchNormalization,Dropout\n",
    "\n",
    "\n",
    "#build a sequential model\n",
    "model= Sequential()\n",
    "#model.add(Input(input_shape=(176,208,3)))\n",
    "\n",
    "#1st conv block #input_shape=(176,176,3)\n",
    "\n",
    "model.add(Conv2D(16,(5,5),activation='relu',strides=(1,1),padding='same',input_shape=(176,208,3)))  #filters multiples of 8 or 16\n",
    "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
    "\n",
    "#2nd conv block\n",
    "model.add(Conv2D(32,(5,5),activation='relu',strides=(2,2),padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#3rd conv block\n",
    "model.add(Conv2D(64,(3,3),activation='relu',strides=(2,2),padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#4th conv block\n",
    "model.add(Conv2D(128,(3,3),activation='relu',strides=(2,2),padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#5th conv block\n",
    "#model.add(Conv2D(256,(3,3),activation='relu',strides=(2,2),padding='same'))\n",
    "#model.add(MaxPool2D(pool_size=(2,2),padding='valid'))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#Ann block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=500,activation='relu'))#128,256\n",
    "model.add(Dense(units=500,activation='relu'))#multiple s of 16 start with 64 add 5layers\n",
    "model.add(Dense(units=500,activation='relu'))\n",
    "model.add(Dense(units=500,activation='relu'))\n",
    "model.add(Dense(units=500,activation='relu'))\n",
    "model.add(Dropout(0.25))  #25% deopout means it will delete the 25% of neurons i.e reduce the features ,this is to avoid the overfitting\n",
    "\n",
    "#outputlayer\n",
    "model.add(Dense(units=4,activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2a467c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "257/257 [==============================] - 81s 311ms/step - loss: 1.1761 - accuracy: 0.4575 - val_loss: 1.0259 - val_accuracy: 0.5005\n",
      "Epoch 2/100\n",
      "257/257 [==============================] - 77s 298ms/step - loss: 1.0663 - accuracy: 0.4732 - val_loss: 1.5330 - val_accuracy: 0.5044\n",
      "Epoch 3/100\n",
      "257/257 [==============================] - 77s 298ms/step - loss: 0.9888 - accuracy: 0.5212 - val_loss: 1.1298 - val_accuracy: 0.5064\n",
      "Epoch 4/100\n",
      "257/257 [==============================] - 76s 295ms/step - loss: 0.8855 - accuracy: 0.5781 - val_loss: 1.3233 - val_accuracy: 0.4995\n",
      "Epoch 5/100\n",
      "257/257 [==============================] - 76s 296ms/step - loss: 0.7674 - accuracy: 0.6606 - val_loss: 1.2655 - val_accuracy: 0.4575\n",
      "Epoch 6/100\n",
      "257/257 [==============================] - 75s 293ms/step - loss: 0.6892 - accuracy: 0.7333 - val_loss: 0.9287 - val_accuracy: 0.6735\n",
      "Epoch 7/100\n",
      "257/257 [==============================] - 77s 298ms/step - loss: 0.5388 - accuracy: 0.7762 - val_loss: 1.0144 - val_accuracy: 0.6168\n",
      "Epoch 8/100\n",
      "257/257 [==============================] - 77s 299ms/step - loss: 0.4595 - accuracy: 0.8106 - val_loss: 1.9928 - val_accuracy: 0.5269\n",
      "Epoch 9/100\n",
      "257/257 [==============================] - 77s 300ms/step - loss: 0.3348 - accuracy: 0.8795 - val_loss: 2.2055 - val_accuracy: 0.5973\n",
      "Epoch 10/100\n",
      "257/257 [==============================] - 78s 302ms/step - loss: 0.2863 - accuracy: 0.9080 - val_loss: 1.4475 - val_accuracy: 0.6266\n",
      "Epoch 11/100\n",
      "257/257 [==============================] - 78s 303ms/step - loss: 0.2587 - accuracy: 0.9095 - val_loss: 1.3084 - val_accuracy: 0.6569\n",
      "Epoch 12/100\n",
      "257/257 [==============================] - 78s 303ms/step - loss: 0.1748 - accuracy: 0.9441 - val_loss: 1.9672 - val_accuracy: 0.7028\n",
      "Epoch 13/100\n",
      "257/257 [==============================] - 77s 301ms/step - loss: 0.1915 - accuracy: 0.9380 - val_loss: 2.3802 - val_accuracy: 0.5748\n",
      "Epoch 14/100\n",
      "257/257 [==============================] - 78s 302ms/step - loss: 0.1426 - accuracy: 0.9497 - val_loss: 2.0542 - val_accuracy: 0.6608\n",
      "Epoch 15/100\n",
      "257/257 [==============================] - 78s 303ms/step - loss: 0.0938 - accuracy: 0.9727 - val_loss: 1.8825 - val_accuracy: 0.6510\n",
      "Epoch 16/100\n",
      "257/257 [==============================] - 78s 304ms/step - loss: 0.1062 - accuracy: 0.9719 - val_loss: 1.3371 - val_accuracy: 0.6725\n",
      "Epoch 17/100\n",
      "257/257 [==============================] - 80s 313ms/step - loss: 0.0758 - accuracy: 0.9763 - val_loss: 1.5595 - val_accuracy: 0.5992\n",
      "Epoch 18/100\n",
      "257/257 [==============================] - 76s 297ms/step - loss: 0.0577 - accuracy: 0.9858 - val_loss: 2.6826 - val_accuracy: 0.5885\n",
      "Epoch 19/100\n",
      "257/257 [==============================] - 77s 298ms/step - loss: 0.0724 - accuracy: 0.9824 - val_loss: 1.9131 - val_accuracy: 0.6618\n",
      "Epoch 20/100\n",
      "257/257 [==============================] - 81s 314ms/step - loss: 0.0351 - accuracy: 0.9900 - val_loss: 1.8897 - val_accuracy: 0.6940\n",
      "Epoch 21/100\n",
      "257/257 [==============================] - 1785s 7s/step - loss: 0.0996 - accuracy: 0.9790 - val_loss: 1.2558 - val_accuracy: 0.7009\n",
      "Epoch 22/100\n",
      "257/257 [==============================] - 74s 289ms/step - loss: 0.0542 - accuracy: 0.9866 - val_loss: 1.5877 - val_accuracy: 0.6305\n",
      "Epoch 23/100\n",
      "257/257 [==============================] - 74s 289ms/step - loss: 0.1139 - accuracy: 0.9717 - val_loss: 2.7887 - val_accuracy: 0.5435\n",
      "Epoch 24/100\n",
      "257/257 [==============================] - 80s 311ms/step - loss: 0.0875 - accuracy: 0.9783 - val_loss: 2.0543 - val_accuracy: 0.5914\n",
      "Epoch 25/100\n",
      "257/257 [==============================] - 73s 284ms/step - loss: 0.0872 - accuracy: 0.9736 - val_loss: 1.5716 - val_accuracy: 0.7087\n",
      "Epoch 26/100\n",
      "257/257 [==============================] - 79s 309ms/step - loss: 0.0556 - accuracy: 0.9854 - val_loss: 1.7597 - val_accuracy: 0.6227\n",
      "Epoch 27/100\n",
      "257/257 [==============================] - 79s 306ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 1.5554 - val_accuracy: 0.6725\n",
      "Epoch 28/100\n",
      "257/257 [==============================] - 80s 311ms/step - loss: 0.0353 - accuracy: 0.9900 - val_loss: 2.8774 - val_accuracy: 0.6422\n",
      "Epoch 29/100\n",
      "257/257 [==============================] - 80s 311ms/step - loss: 0.0864 - accuracy: 0.9827 - val_loss: 3.0602 - val_accuracy: 0.4643\n",
      "Epoch 30/100\n",
      "257/257 [==============================] - 79s 309ms/step - loss: 0.1395 - accuracy: 0.9610 - val_loss: 2.1122 - val_accuracy: 0.6119\n",
      "Epoch 31/100\n",
      "257/257 [==============================] - 79s 308ms/step - loss: 0.0484 - accuracy: 0.9888 - val_loss: 4.4118 - val_accuracy: 0.5611\n",
      "Epoch 32/100\n",
      "257/257 [==============================] - 72s 280ms/step - loss: 0.0754 - accuracy: 0.9815 - val_loss: 3.9124 - val_accuracy: 0.5552\n",
      "Epoch 33/100\n",
      "257/257 [==============================] - 79s 307ms/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 1.8839 - val_accuracy: 0.6686\n",
      "Epoch 34/100\n",
      "257/257 [==============================] - 79s 307ms/step - loss: 0.0295 - accuracy: 0.9924 - val_loss: 2.5113 - val_accuracy: 0.6774\n",
      "Epoch 35/100\n",
      "257/257 [==============================] - 946s 4s/step - loss: 0.0396 - accuracy: 0.9900 - val_loss: 2.6177 - val_accuracy: 0.6676\n",
      "Epoch 36/100\n",
      "257/257 [==============================] - 79s 309ms/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 2.6220 - val_accuracy: 0.6452\n",
      "Epoch 37/100\n",
      "257/257 [==============================] - 79s 306ms/step - loss: 0.0261 - accuracy: 0.9929 - val_loss: 3.2327 - val_accuracy: 0.6149\n",
      "Epoch 38/100\n",
      "257/257 [==============================] - 79s 307ms/step - loss: 0.1298 - accuracy: 0.9671 - val_loss: 1.5269 - val_accuracy: 0.6872\n",
      "Epoch 39/100\n",
      "257/257 [==============================] - 80s 312ms/step - loss: 0.0413 - accuracy: 0.9907 - val_loss: 1.5643 - val_accuracy: 0.6305\n",
      "Epoch 40/100\n",
      "257/257 [==============================] - 79s 307ms/step - loss: 0.0443 - accuracy: 0.9910 - val_loss: 1.6213 - val_accuracy: 0.6588\n",
      "Epoch 41/100\n",
      "257/257 [==============================] - 79s 308ms/step - loss: 0.0413 - accuracy: 0.9907 - val_loss: 1.9287 - val_accuracy: 0.6364\n",
      "Epoch 42/100\n",
      "257/257 [==============================] - 7861s 31s/step - loss: 0.0348 - accuracy: 0.9885 - val_loss: 1.3415 - val_accuracy: 0.6804\n",
      "Epoch 43/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 0.0943 - accuracy: 0.9819 - val_loss: 2.5156 - val_accuracy: 0.5523\n",
      "Epoch 44/100\n",
      "257/257 [==============================] - 56s 216ms/step - loss: 0.0454 - accuracy: 0.9876 - val_loss: 4.4887 - val_accuracy: 0.5122\n",
      "Epoch 45/100\n",
      "257/257 [==============================] - 55s 212ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 6.1888 - val_accuracy: 0.5024\n",
      "Epoch 46/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 2.0179 - val_accuracy: 0.6999\n",
      "Epoch 47/100\n",
      "257/257 [==============================] - 54s 209ms/step - loss: 0.0210 - accuracy: 0.9949 - val_loss: 2.5773 - val_accuracy: 0.6442\n",
      "Epoch 48/100\n",
      "257/257 [==============================] - 54s 208ms/step - loss: 0.0371 - accuracy: 0.9910 - val_loss: 1.6495 - val_accuracy: 0.6970\n",
      "Epoch 49/100\n",
      "257/257 [==============================] - 56s 218ms/step - loss: 0.0120 - accuracy: 0.9968 - val_loss: 2.1915 - val_accuracy: 0.6950\n",
      "Epoch 50/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 0.0809 - accuracy: 0.9861 - val_loss: 2.2894 - val_accuracy: 0.6413\n",
      "Epoch 51/100\n",
      "257/257 [==============================] - 56s 219ms/step - loss: 0.0524 - accuracy: 0.9910 - val_loss: 1.8006 - val_accuracy: 0.6334\n",
      "Epoch 52/100\n",
      "257/257 [==============================] - 56s 219ms/step - loss: 0.1042 - accuracy: 0.9700 - val_loss: 1.6651 - val_accuracy: 0.6725\n",
      "Epoch 53/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 0.0329 - accuracy: 0.9905 - val_loss: 1.3551 - val_accuracy: 0.6745\n",
      "Epoch 54/100\n",
      "257/257 [==============================] - 56s 218ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 5.0256 - val_accuracy: 0.6422\n",
      "Epoch 55/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 2.6866 - val_accuracy: 0.6579\n",
      "Epoch 56/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 2.3005 - val_accuracy: 0.6804\n",
      "Epoch 57/100\n",
      "257/257 [==============================] - 58s 224ms/step - loss: 0.0210 - accuracy: 0.9959 - val_loss: 1.6267 - val_accuracy: 0.7556\n",
      "Epoch 58/100\n",
      "257/257 [==============================] - 56s 217ms/step - loss: 0.0353 - accuracy: 0.9917 - val_loss: 1.9049 - val_accuracy: 0.6716\n",
      "Epoch 59/100\n",
      "257/257 [==============================] - 58s 224ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 3.2967 - val_accuracy: 0.7273\n",
      "Epoch 60/100\n",
      "257/257 [==============================] - 57s 224ms/step - loss: 0.1102 - accuracy: 0.9795 - val_loss: 1.5330 - val_accuracy: 0.6940\n",
      "Epoch 61/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 0.0420 - accuracy: 0.9883 - val_loss: 2.3933 - val_accuracy: 0.7077\n",
      "Epoch 62/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 0.0278 - accuracy: 0.9946 - val_loss: 2.0419 - val_accuracy: 0.7107\n",
      "Epoch 63/100\n",
      "257/257 [==============================] - 56s 218ms/step - loss: 0.0170 - accuracy: 0.9971 - val_loss: 2.4097 - val_accuracy: 0.6852\n",
      "Epoch 64/100\n",
      "257/257 [==============================] - 56s 220ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 1.8197 - val_accuracy: 0.7390\n",
      "Epoch 65/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 2.8670 - val_accuracy: 0.7067\n",
      "Epoch 66/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 5.0798e-05 - accuracy: 1.0000 - val_loss: 3.2596 - val_accuracy: 0.7038\n",
      "Epoch 67/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 4.4287e-05 - accuracy: 1.0000 - val_loss: 3.4378 - val_accuracy: 0.7067\n",
      "Epoch 68/100\n",
      "257/257 [==============================] - 58s 224ms/step - loss: 6.3882e-06 - accuracy: 1.0000 - val_loss: 3.4993 - val_accuracy: 0.7165\n",
      "Epoch 69/100\n",
      "257/257 [==============================] - 56s 219ms/step - loss: 3.0351e-06 - accuracy: 1.0000 - val_loss: 3.8372 - val_accuracy: 0.7058\n",
      "Epoch 70/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 1.6750e-06 - accuracy: 1.0000 - val_loss: 3.9731 - val_accuracy: 0.7165\n",
      "Epoch 71/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 1.8925e-06 - accuracy: 1.0000 - val_loss: 4.6186 - val_accuracy: 0.6960\n",
      "Epoch 72/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 1.5303e-06 - accuracy: 1.0000 - val_loss: 4.7337 - val_accuracy: 0.6960\n",
      "Epoch 73/100\n",
      "257/257 [==============================] - 56s 217ms/step - loss: 1.3933e-06 - accuracy: 1.0000 - val_loss: 5.0672 - val_accuracy: 0.6931\n",
      "Epoch 74/100\n",
      "257/257 [==============================] - 56s 219ms/step - loss: 4.0751e-07 - accuracy: 1.0000 - val_loss: 5.1286 - val_accuracy: 0.6960\n",
      "Epoch 75/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 1.7578e-06 - accuracy: 1.0000 - val_loss: 5.7070 - val_accuracy: 0.6921\n",
      "Epoch 76/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 2.1891e-07 - accuracy: 1.0000 - val_loss: 5.4534 - val_accuracy: 0.6950\n",
      "Epoch 77/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 3.7706e-07 - accuracy: 1.0000 - val_loss: 5.3006 - val_accuracy: 0.7087\n",
      "Epoch 78/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 2.4943e-07 - accuracy: 1.0000 - val_loss: 5.6722 - val_accuracy: 0.6950\n",
      "Epoch 79/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 7.1094e-08 - accuracy: 1.0000 - val_loss: 5.2510 - val_accuracy: 0.7087\n",
      "Epoch 80/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 2.1582e-07 - accuracy: 1.0000 - val_loss: 5.7770 - val_accuracy: 0.6960\n",
      "Epoch 81/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 4.3492e-07 - accuracy: 1.0000 - val_loss: 5.6816 - val_accuracy: 0.7038\n",
      "Epoch 82/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 3.5558e-07 - accuracy: 1.0000 - val_loss: 5.6929 - val_accuracy: 0.7087\n",
      "Epoch 83/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 6.3036e-08 - accuracy: 1.0000 - val_loss: 6.3872 - val_accuracy: 0.6911\n",
      "Epoch 84/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 1.0032e-07 - accuracy: 1.0000 - val_loss: 5.9503 - val_accuracy: 0.7067\n",
      "Epoch 85/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 4.9160e-08 - accuracy: 1.0000 - val_loss: 6.5682 - val_accuracy: 0.6931\n",
      "Epoch 86/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 4.3139e-08 - accuracy: 1.0000 - val_loss: 6.2867 - val_accuracy: 0.7028\n",
      "Epoch 87/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 9.5643e-08 - accuracy: 1.0000 - val_loss: 6.0551 - val_accuracy: 0.7146\n",
      "Epoch 88/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 1.6465e-08 - accuracy: 1.0000 - val_loss: 6.4131 - val_accuracy: 0.6960\n",
      "Epoch 89/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 2.7598e-07 - accuracy: 1.0000 - val_loss: 6.1612 - val_accuracy: 0.7155\n",
      "Epoch 90/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 5.1369e-08 - accuracy: 1.0000 - val_loss: 6.5537 - val_accuracy: 0.7028\n",
      "Epoch 91/100\n",
      "257/257 [==============================] - 57s 221ms/step - loss: 2.3010e-08 - accuracy: 1.0000 - val_loss: 6.4422 - val_accuracy: 0.7107\n",
      "Epoch 92/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 1.3876e-08 - accuracy: 1.0000 - val_loss: 6.7669 - val_accuracy: 0.7048\n",
      "Epoch 93/100\n",
      "257/257 [==============================] - 57s 223ms/step - loss: 7.1851e-09 - accuracy: 1.0000 - val_loss: 6.5521 - val_accuracy: 0.7107\n",
      "Epoch 94/100\n",
      "257/257 [==============================] - 57s 220ms/step - loss: 1.2305e-08 - accuracy: 1.0000 - val_loss: 6.4139 - val_accuracy: 0.7175\n",
      "Epoch 95/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 1.3934e-08 - accuracy: 1.0000 - val_loss: 6.3454 - val_accuracy: 0.7146\n",
      "Epoch 96/100\n",
      "257/257 [==============================] - 58s 226ms/step - loss: 3.8980e-09 - accuracy: 1.0000 - val_loss: 6.5624 - val_accuracy: 0.7107\n",
      "Epoch 97/100\n",
      "257/257 [==============================] - 58s 225ms/step - loss: 2.3184e-08 - accuracy: 1.0000 - val_loss: 6.9951 - val_accuracy: 0.7058\n",
      "Epoch 98/100\n",
      "257/257 [==============================] - 56s 220ms/step - loss: 1.4166e-07 - accuracy: 1.0000 - val_loss: 7.1738 - val_accuracy: 0.7038\n",
      "Epoch 99/100\n",
      "257/257 [==============================] - 56s 220ms/step - loss: 8.9014e-09 - accuracy: 1.0000 - val_loss: 6.7419 - val_accuracy: 0.7155\n",
      "Epoch 100/100\n",
      "257/257 [==============================] - 57s 222ms/step - loss: 1.0007e-08 - accuracy: 1.0000 - val_loss: 7.3675 - val_accuracy: 0.7038\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) #SGD optimiser\n",
    "\n",
    "#fit on data for 5 epochs\n",
    "history=model.fit(train,epochs=100,validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18ef7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_56 (Conv2D)          (None, 176, 208, 16)      1216      \n",
      "                                                                 \n",
      " max_pooling2d_54 (MaxPoolin  (None, 88, 104, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 44, 52, 32)        12832     \n",
      "                                                                 \n",
      " max_pooling2d_55 (MaxPoolin  (None, 22, 26, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 22, 26, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 11, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_56 (MaxPoolin  (None, 5, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 5, 6, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_57 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 1, 1, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 500)               64500     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 4)                 2004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,175,800\n",
      "Trainable params: 1,175,352\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
